{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `ipytest` Summary\n",
    "\n",
    "`ipytest` aims to make testing code in IPython notebooks easy. At its core, it offers a way to run pytest tests inside the notebook environment. It is also designed to make the transfer of the tests into proper python modules easy by supporting to use standard `pytest` features.\n",
    "\n",
    "To get started install `ipytest` via:\n",
    "\n",
    "```bash\n",
    "pip install -U ipytest\n",
    "```\n",
    "\n",
    "To use `ipytest`, import it and configure the notebook. In most cases, running `ipytest.autoconfig()` will result in reasonable defaults:\n",
    "\n",
    "- Tests can be executed with the `%%ipytest` magic\n",
    "- The `pytest` assert rewriting system to get nice assert messages will integrated into the notebook \n",
    "\n",
    "For more control, pass the relevant arguments to `ipytest.autconfig()`. For details, see the documentation in the readme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute tests\n",
    "\n",
    "To execute test, just decorate the cells containing the tests with the `%%ipytest` magic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "# define the tests\n",
    "\n",
    "def test_my_func():\n",
    "    assert my_func(0) == 0\n",
    "    assert my_func(1) == 0\n",
    "    assert my_func(2) == 2\n",
    "    assert my_func(3) == 2\n",
    "    \n",
    "    \n",
    "def my_func(x):\n",
    "    return x // 2 * 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pytest fixtures\n",
    "\n",
    "Common pytest features, such as fixtures and parametrize, are supported out of the box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                        [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize('input,expected', [\n",
    "    (0, 0),\n",
    "    (1, 0),\n",
    "    (2, 2),\n",
    "    (3, 2),\n",
    "])\n",
    "def test_parametrized(input, expected):\n",
    "    assert my_func(input) == expected\n",
    "    \n",
    "    \n",
    "@pytest.fixture\n",
    "def my_fixture():\n",
    "    return 42\n",
    "    \n",
    "    \n",
    "def test_fixture(my_fixture):\n",
    "    assert my_fixture == 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pytest](https://pytest.org) offers a [extensive options](https://docs.pytest.org/en/latest/how-to/usage.html#specifying-which-tests-to-run) to select subsets of tests to run. They can be also used in conjunction with `ipytest`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                           [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m2 deselected\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -k feature1\n",
    "\n",
    "def test_feature1_test1():\n",
    "    assert True\n",
    "\n",
    "def test_feature1_test2():\n",
    "    assert True\n",
    "    \n",
    "def test_feature2_test1():\n",
    "    assert False\n",
    "    \n",
    "def test_feature2_test2():\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests can also be selected based on node ids. The notebook can be referenced via the special `{MODULE}` name. For example `{MODULE}::test_feature1_test1` references the corresponding test defined in the notebook: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest {MODULE}::test_feature1_test1\n",
    "        \n",
    "def test_feature1_test1():\n",
    "    assert True\n",
    "\n",
    "def test_feature1_test2():\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Deselection](https://docs.pytest.org/en/7.1.x/example/pythoncollection.html#deselect-tests-during-test-collection) works as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m1 deselected\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest --deselect {MODULE}::test_feature1_test2\n",
    "\n",
    "def test_feature1_test1():\n",
    "    assert True\n",
    "\n",
    "def test_feature1_test2():\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging failed tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [debugging functionality of pytest](https://docs.pytest.org/en/latest/how-to/failures.html) can be used as well. For example, to debug the first failed test (and then stop the pytest run) use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mF\u001b[0m\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> traceback >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_example\u001b[39;49;00m():\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[94m10\u001b[39;49;00m):\n",
      "            \u001b[94mif\u001b[39;49;00m i == \u001b[94m5\u001b[39;49;00m:\n",
      ">               \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(i)\n",
      "\u001b[1m\u001b[31mE               ValueError: 5\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/tmp/ipykernel_10676/636047835.py\u001b[0m:4: ValueError\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> entering PDB >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>> PDB post_mortem (IO-capturing turned off) >>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "> \u001b[0;32m/tmp/ipykernel_10676/636047835.py\u001b[0m(4)\u001b[0;36mtest_example\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      1 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mtest_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      2 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      3 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 4 \u001b[0;31m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  p i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================== short test summary info ======================================\n",
      "FAILED tmp5v4c9y11.py::test_example - ValueError: 5\n",
      "\u001b[31m!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! stopping after 1 failures !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\u001b[0m\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!! _pytest.outcomes.Exit: Quitting debugger !!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 3.38s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -x --pdb\n",
    "\n",
    "def test_example():\n",
    "    for i in range(10):\n",
    "        if i == 5: \n",
    "            raise ValueError(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "NBVAL_RAISES_EXCEPTION"
    ]
   },
   "source": [
    "## Checking notebooks automatically\n",
    "\n",
    "`ipytest` itself does not support validating notebooks in a programmatic fashion. For this task, the [`nbval` package](https://nbval.readthedocs.io/en/latest) can be used. In its default configuration, `ipytest` will not raise lead to execution failures, but only display the exception. While this behavior is helpful during interactive development, it will prevent `nbval` from catching errors. To raise errors if any test fails, configure `raise_on_error=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbval-raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "# ipytest: raise_on_error=True\n",
    "\n",
    "def test():\n",
    "    raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
